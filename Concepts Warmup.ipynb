{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Neuron implementation from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(output):\n",
    "    return np.maximum(0,output)\n",
    "\n",
    "weight = np.random.rand()\n",
    "bias = np.random.randint(1,10)\n",
    "input = np.random.randint(1,100)\n",
    "\n",
    "output = np.dot(weight, input) + bias\n",
    "print(relu(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6 ]\n",
      " [6.89]\n",
      " [8.07]]\n"
     ]
    }
   ],
   "source": [
    "weights = [[.3,.6,.7],\n",
    "[.2,.54,.87],\n",
    "[.96,.23,.55]]\n",
    "\n",
    "inputs = [[1],\n",
    "[2],\n",
    "[3]]\n",
    "\n",
    "bias = [[2],\n",
    "[3],\n",
    "[5]]\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "print(relu(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple 3 Layered Neural Network (Forward Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "Inputs for 1st layer:\n",
      " [[ 9.12  10.384 11.045]]\n",
      "Inputs for 2nd layer:\n",
      " [[11.3353   21.645588]]\n",
      "Final output:\n",
      " 30.37543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_original = np.array([[1],[2],[3],[4],[5]]) #(5x1)\n",
    "\n",
    "#(5x3) matrix: 5 inputs, 3 neurons\n",
    "weights_layer1 = np.hstack([\n",
    "   np.array([[.3],[.2],[.5],[.23],[.6]]), #Neuron 1 Weights\n",
    "   np.array([[.8],[.232],[.9],[.53],[.66]]), #Neuron 2 Weights\n",
    "   np.array([[.32],[.27],[.655],[.23],[.46]]) #Neuron 3 Weights\n",
    "]) \n",
    "\n",
    "#(3x2) matrix: 3 inputs, 2 neurons\n",
    "weights_layer2 = np.hstack([\n",
    "   np.array([[.3],[.2],[.5]]), #Neuron 1 Weights\n",
    "   np.array([[.8],[.232],[.9]]), #Neuron 2 Weights\n",
    "]) \n",
    "\n",
    "#(2x1), 2 inputs for 1 neuron\n",
    "weight_final = np.hstack([\n",
    "   np.array([[0.63],[.75]]) #Final Neuron Weights\n",
    "])\n",
    "\n",
    "bias_layer1 = np.array([[3],[1],[5]]) #(3x1)\n",
    "bias_layer2 = np.array([[1],[2]]) #(2x1)\n",
    "bias_final = np.array([[7]])\n",
    "\n",
    "def hidden_layer1(inputs, weights, bias):\n",
    "   #(1x5)*(5x3) + (1x3), (inputs^T * weights) + bias^T\n",
    "   output = np.dot(inputs.T, weights) + bias.T #(1x3)\n",
    "   return np.maximum(0,output)\n",
    "\n",
    "def hidden_layer2(inputs, weights, bias):\n",
    "   #(1x3)*(3x2) + (1x3), (inputs^T * weights) + bias^T\n",
    "   output = np.dot(inputs, weights) + bias.T #(1x3)\n",
    "   return np.maximum(0,output)\n",
    "\n",
    "def final_layer(inputs, weights, bias):\n",
    "   #(2x1)*(1x1) + (1x1)\n",
    "   output = np.dot(inputs, weights) + bias\n",
    "   return np.maximum(0,output)\n",
    "print(\"Inputs:\\n\",input_original)\n",
    "print(\"Inputs for 1st layer:\\n\",hidden_layer1(input_original, weights_layer1, bias_layer1))\n",
    "inputs_2 = np.array(hidden_layer1(input_original, weights_layer1, bias_layer1))\n",
    "print(\"Inputs for 2nd layer:\\n\",hidden_layer2(inputs_2, weights_layer2, bias_layer2))\n",
    "inputs_final = np.array(hidden_layer2(inputs_2, weights_layer2, bias_layer2)) \n",
    "output_final = final_layer(inputs_final, weight_final, bias_final)\n",
    "print(\"Final output:\\n\",output_final.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Layered Back-Propagation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Forward Pass ==================\n",
      "1st Layer output:\n",
      " [[3.99 3.29 2.59]] \n",
      "\n",
      "2st Layer output:\n",
      " [[5.4552 6.4972]] \n",
      "\n",
      "Final Layer output:\n",
      " 7.188 \n",
      "\n",
      "================== Backward Pass ==================\n",
      "Error(Actual-Predicted):  -6.188\n",
      "\n",
      "Updated Weight & Bias of 1st Hidden Layer:\n",
      " [[ 0.1570487   0.03662708  0.00382068]\n",
      " [ 0.0540974  -0.09674583 -0.36235864]\n",
      " [ 0.2411461  -0.28011875 -1.07853796]] [[0.29749646]\n",
      " [1.29749646]\n",
      " [0.29749646]]\n",
      "\n",
      "Updated Weight & Bias of 2nd Hidden Layer:\n",
      " [[-0.51482241 -1.42964483]\n",
      " [-0.33187111 -1.11374223]\n",
      " [-0.08891981 -0.30783963]] [[1.38735157]\n",
      " [2.38735157]]\n",
      "\n",
      "Updated Weight & Bias of Final Layer:\n",
      " [[-3.04587851]\n",
      " [-3.36070646]] [0.4811632]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#        ================== Forward Pass ================== \n",
    "''' \n",
    "Feed Forward Multi Layer Neural Network:\n",
    "1. y = mx + c | no = w x i + b | neuron_output = weights x inputs + bias\n",
    "2. activation_function(neuron_output)\n",
    "'''\n",
    "\n",
    "#(3x1)\n",
    "inputs_original = np.array([[1],[2],[3]]) \n",
    "\n",
    "#(3x3)\n",
    "weights_layer1 = np.hstack([\n",
    "   np.array([[.3],[.34],[.67]]), # Weights for Neuron 1, (3x1)\n",
    "   np.array([[.2],[.23],[.21]]), # Weights for Neuron 2, (3x1)\n",
    "   np.array([[.4],[.43],[.11]]) # Weights for Neuron 3, (3x1)\n",
    "])\n",
    "\n",
    "#(3x2)\n",
    "weights_layer2 = np.hstack([\n",
    "   np.array([[.3],[.34],[.44]]), # Weights for Neuron 1, (3x1)\n",
    "   np.array([[.2],[.23],[.75]]), # Weights for Neuron 2, (3x1)\n",
    "])\n",
    "\n",
    "#(2x1)\n",
    "weight_final = np.array([[.33],[.66]]) # Weight for final Neuron\n",
    "\n",
    "bias_layer1 = np.array([[1],[2],[1]]) # Biases for Hidden 1st Layer, (3x1)\n",
    "bias_layer2 = np.array([[2],[3]]) # Biases for Hidden 2nd Layer, (2x1)\n",
    "bias_final = np.array([1.1]) # Bias for Final Layer, (1x1)\n",
    "\n",
    "def hidden_layer1(inputs, weights, bias):\n",
    "   # (3x1)*(3x3) + (1x3)\n",
    "   output = np.dot(inputs.T, weights) + bias.T\n",
    "   return output, np.maximum(0, output)\n",
    "\n",
    "def hidden_layer2(inputs, weights, bias):\n",
    "   # (1x3)*(3x2) + (1x2)\n",
    "   output = np.dot(inputs, weights) + bias.T\n",
    "   return output, np.maximum(0, output)\n",
    "\n",
    "def final_layer(inputs, weights, bias):\n",
    "   # (1x2)*(2x1) + (1x1)\n",
    "   output = np.dot(inputs, weights) + bias\n",
    "   return output, output\n",
    "\n",
    "# 'z' represents raw neuron output without activation function\n",
    "output_layer_1, z1 = hidden_layer1(inputs_original, weights_layer1, bias_layer1)\n",
    "output_layer_2, z2 = hidden_layer2(output_layer_1, weights_layer2, bias_layer2)\n",
    "final_layer, z_final = final_layer(output_layer_2, weight_final, bias_final)\n",
    "print(\"================== Forward Pass ==================\")\n",
    "print(\"1st Layer output:\\n\", output_layer_1,\"\\n\")\n",
    "print(\"2st Layer output:\\n\", output_layer_2,\"\\n\")\n",
    "print(\"Final Layer output:\\n\", final_layer.squeeze().round(3),'\\n')\n",
    "\n",
    "\n",
    "print(\"================== Backward Pass ==================\")  \n",
    "'''\n",
    "\n",
    "==================================== Back Propagation =====================================\n",
    "1. Loss Function (Mean Squared Error):\n",
    "   l = 1/2(a - p)^2 | loss = 0.5(actual - predicted)^2\n",
    "\n",
    "2. Gradients:\n",
    "Note: Weights, Biases, Final Output are derivated/unpacked to update wrt to gradient/changes/difference between actual & predicted.\n",
    "\n",
    "   [Final/Output Layer]\n",
    "      Loss w.r.t Output: (Loss Function Derivative)\n",
    "         dL/dp = (p - a) | d_loss_output = predicted - actual\n",
    "      Predicted w.r.t Weight:\n",
    "         dp/dw = A | d_predicted_weight = neuron_output        ### Note: You can use 'final_output' in place of 'neuron_output' ###\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = A x (dL/dp) | d_loss_weight = neuron_output x d_loss_output\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dp) | d_loss_bias = sum(loss_output)\n",
    "\n",
    "   [Hidden Layer]\n",
    "      Loss w.r.t  Pre-Activation Neuron Output (Z):\n",
    "         dL/dZ = dL/dA x dA/dZ = (dL/dp x w).ReLu(Z) | d_loss_raw_neuron_output = (d_loss_output x weight).activation_function(d_raw_neuron_output)\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = A x (dL/dZ) | d_loss_weight = neuron_output x (d_loss_raw_neuron_output)\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dZ) | d_loss_bias = sum(d_loss_raw_neuron_output)\n",
    "   \n",
    "   [Input Layer]\n",
    "      Loss w.r.t Pre-Activation Neuron Output (Z):\n",
    "         dL/dZ = (dL/dZ x w).ReLu(Z) | d_loss_raw_neuron_output = (d_loss_raw_neuron_output x weight).activation_function(d_raw_neuron_output)\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = i x dL/dZ | d_loss_weight = input x d_loss_raw_neuron_output\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dZ) | d_loss_bias = sum(d_loss_raw_neuron_output)\n",
    "\n",
    "   [Update Weights & Biases]\n",
    "      w = w - (lr x dL/dw) | weight = weight - learning_rate x d_loss_weight\n",
    "      b = b - (lr x dL/db) | bias = bias - learning_rate x d_loss_weight\n",
    "'''\n",
    "\n",
    "\n",
    "def ReLu_derivative(neuron):\n",
    "   return (neuron > 0).astype(float)\n",
    "    \n",
    "def bp_layer_final(A_prev, predicted, actual):\n",
    "   '''   \n",
    "   [Final/Output Layer]\n",
    "      Loss w.r.t Predicted or Output: (Loss Function Derivative)\n",
    "         dL/dp = (p - a)\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = A x (dL/dp)\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dp)\n",
    "   '''\n",
    "   '''\n",
    "    [Final/Output Layer]\n",
    "      Inputs:\n",
    "        A_prev: Activation from previous layer (shape: 1xN)\n",
    "        predicted: Predicted output (shape: 1x1)\n",
    "        actual: Actual value (scalar)\n",
    "    '''\n",
    "   # Loss gradient w.r.t. predicted output (dL/dp) \n",
    "   dL_dp = predicted - actual \n",
    "\n",
    "   # Gradient for weights (dL/dw) = A_prev^T @ dL_dp\n",
    "   dL_dw = A_prev.T @ dL_dp\n",
    "   \n",
    "   # Gradient for bias (dL/db) = sum(dL_dp)\n",
    "   dL_db = np.sum(dL_dp)\n",
    "   return dL_dw, dL_db\n",
    "\n",
    "def bp_layer_hidden(A_prev, dL_dz_next, weight_next, z_current):\n",
    "   '''   \n",
    "   [Hidden Layer]\n",
    "      Loss w.r.t  Pre-Activation/Raw Neuron Output (Z):\n",
    "         dL/dZ = dL/dA x dA/dZ = (dL/dp x w).ReLu(Z)\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = A x (dL/dZ)\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dZ)\n",
    "   '''\n",
    "   '''\n",
    "    [Hidden Layer]\n",
    "      Inputs:\n",
    "        A_prev: Activation from previous layer (shape: 1xM)\n",
    "        dL_dz_next: Gradient from the next layer (shape: 1xN)\n",
    "        weight_next: Weights of the next layer (shape: MxN)\n",
    "        z_current: Pre-activation of the current layer (shape: 1xM)\n",
    "    '''\n",
    "   # Gradient w.r.t. pre-activation (dL/dz_current)\n",
    "   dL_dz_current = (dL_dz_next @ weight_next.T) * ReLu_derivative(z_current) # (1xR)\n",
    "\n",
    "   # Gradient for weights (dL/dw) = A_prev^T @ dL_dz_current\n",
    "   dL_dw = A_prev.T @ dL_dz_current # (RxR)\n",
    "\n",
    "   # Gradient for bias (dL/db) = sum(dL_dz_current)\n",
    "   dL_db = np.sum(dL_dz_current)\n",
    "   return dL_dw, dL_db, dL_dz_current\n",
    "\n",
    "def bp_layer_input(input_original, dL_dz_next, weight_next, z_current):\n",
    "   '''   \n",
    "   [Input Layer]\n",
    "      Loss w.r.t Pre-Activation Neuron Output (Z):\n",
    "         dL/dZ = (dL/dZ x w).ReLu(Z)\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = i x dL/dZ\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dZ)\n",
    "   '''\n",
    "   '''\n",
    "    [Input Layer]\n",
    "      Inputs:\n",
    "        input: Input data (shape: 1xD)\n",
    "        dL_dz_next: Gradient from the next layer (shape: 1xM)\n",
    "        weight_next: Weights of the next layer (shape: DxM)\n",
    "        z_current: Pre-activation of the input layer (shape: 1xD)\n",
    "    '''\n",
    "   # Gradient w.r.t. pre-activation (dL/dz_current)\n",
    "   dL_dz_current = (dL_dz_next @ weight_next.T) * ReLu_derivative(z_current)\n",
    "\n",
    "   # Gradient for weights (dL/dW) = input.T @ dL_dz_current\n",
    "   dL_dw = input_original.T @ dL_dz_current\n",
    "\n",
    "    # Gradient for bias (dL/db) = sum(dL_dz_current)\n",
    "   dL_db = np.sum(dL_dz_current)\n",
    "   return dL_dw, dL_db\n",
    "\n",
    "def update_weights(weight, bias, dL_dw, dL_db, learning_rate):\n",
    "   '''   \n",
    "   [Update Weights]\n",
    "      w = w - (lr x dL/dw)\n",
    "      b = b - (lr x dL/db)\n",
    "   '''\n",
    "   '''Update weights and biases using gradients.'''\n",
    "\n",
    "   weight = weight - (learning_rate * dL_dw)\n",
    "   bias = bias - (learning_rate * dL_db)\n",
    "   return weight, bias\n",
    "\n",
    "\n",
    "# Implementing back propagation\n",
    "learning_rate = 0.01\n",
    "actual_output = 1\n",
    "\n",
    "predicted_output = final_layer.round(3).squeeze()\n",
    "error = actual_output - predicted_output\n",
    "print(\"Error(Actual-Predicted): \",error)\n",
    "\n",
    "# Computing loss gradient (dL/dy_pred)\n",
    "dL_dp = (final_layer - actual_output) * ReLu_derivative(z_final) # final_layer is the predicted output\n",
    "\n",
    "# Storing gradients for final layer weights and biases\n",
    "dL_dw_final, dL_db_final = bp_layer_final(\n",
    "   A_prev=output_layer_2, # Activation from previous (hidden) layer\n",
    "   predicted=final_layer, \n",
    "   actual=actual_output\n",
    ")\n",
    "\n",
    "# Storing gradients for hidden layer2 weights and biases\n",
    "dL_dw_layer2, dL_db_layer2, dL_dz_layer2 = bp_layer_hidden(\n",
    "   A_prev=output_layer_1, # Activation from previous layer (hidden layer 1)\n",
    "   dL_dz_next=dL_dp, # Gradient from final layer\n",
    "   weight_next=weight_final, # Weights of the next layer (final layer)\n",
    "   z_current=z2 # Pre-activation of hidden layer 2 (Z2)\n",
    ")\n",
    "\n",
    "# Storing gradients for hidden layer1 weights and biases\n",
    "dL_dw_layer1, dL_db_layer1, dL_dz_layer1 = bp_layer_hidden(\n",
    "   A_prev=inputs_original.T, # Input data (transposed to match dimensions)\n",
    "   dL_dz_next=dL_dz_layer2,  # Gradient from hidden layer 2\n",
    "   weight_next=weights_layer2, # Weights of the next layer (hidden layer 2)\n",
    "   z_current=z1 # Pre-activation of hidden layer 1 (Z1)\n",
    ")\n",
    "\n",
    "# Updating final layer weights and biases\n",
    "weight_final, bias_final = update_weights(\n",
    "   weight_final, \n",
    "   bias_final,\n",
    "   dL_dw_final,\n",
    "   dL_db_final,\n",
    "   learning_rate\n",
    ")\n",
    "\n",
    "# Updating hidden layer2 weights and biases\n",
    "weights_layer2, bias_layer2 = update_weights(\n",
    "   weights_layer2,\n",
    "   bias_layer2,\n",
    "   dL_dw_layer2,\n",
    "   dL_db_layer2,\n",
    "   learning_rate\n",
    ")\n",
    "\n",
    "# Updating first hidden layer weights and biases\n",
    "weights_layer1, bias_layer1 = update_weights(\n",
    "   weights_layer1,\n",
    "   bias_layer1,\n",
    "   dL_dw_layer1,\n",
    "   dL_db_layer1,\n",
    "   learning_rate\n",
    ")\n",
    "\n",
    "print(\"\\nUpdated Weight & Bias of 1st Hidden Layer:\\n\", weights_layer1, bias_layer1)\n",
    "print(\"\\nUpdated Weight & Bias of 2nd Hidden Layer:\\n\", weights_layer2, bias_layer2)\n",
    "print(\"\\nUpdated Weight & Bias of Final Layer:\\n\", weight_final, bias_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
