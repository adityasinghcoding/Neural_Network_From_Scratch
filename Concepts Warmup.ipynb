{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Neuron implementation from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(output):\n",
    "    return np.maximum(0,output)\n",
    "\n",
    "weight = np.random.rand()\n",
    "bias = np.random.randint(1,10)\n",
    "input = np.random.randint(1,100)\n",
    "\n",
    "output = np.dot(weight, input) + bias\n",
    "print(relu(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6 ]\n",
      " [6.89]\n",
      " [8.07]]\n"
     ]
    }
   ],
   "source": [
    "weights = [[.3,.6,.7],\n",
    "[.2,.54,.87],\n",
    "[.96,.23,.55]]\n",
    "\n",
    "inputs = [[1],\n",
    "[2],\n",
    "[3]]\n",
    "\n",
    "bias = [[2],\n",
    "[3],\n",
    "[5]]\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "print(relu(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple 3 Layered Neural Network (Forward Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "Inputs for 1st layer:\n",
      " [[ 9.12  10.384 11.045]]\n",
      "Inputs for 2nd layer:\n",
      " [[11.3353   21.645588]]\n",
      "Final output:\n",
      " 30.37543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_original = np.array([[1],[2],[3],[4],[5]]) #(5x1)\n",
    "\n",
    "#(5x3) matrix: 5 inputs, 3 neurons\n",
    "weights_layer1 = np.hstack([\n",
    "   np.array([[.3],[.2],[.5],[.23],[.6]]), #Neuron 1 Weights\n",
    "   np.array([[.8],[.232],[.9],[.53],[.66]]), #Neuron 2 Weights\n",
    "   np.array([[.32],[.27],[.655],[.23],[.46]]) #Neuron 3 Weights\n",
    "]) \n",
    "\n",
    "#(3x2) matrix: 3 inputs, 2 neurons\n",
    "weights_layer2 = np.hstack([\n",
    "   np.array([[.3],[.2],[.5]]), #Neuron 1 Weights\n",
    "   np.array([[.8],[.232],[.9]]), #Neuron 2 Weights\n",
    "]) \n",
    "\n",
    "#(2x1), 2 inputs for 1 neuron\n",
    "weight_final = np.hstack([\n",
    "   np.array([[0.63],[.75]]) #Final Neuron Weights\n",
    "])\n",
    "\n",
    "bias_layer1 = np.array([[3],[1],[5]]) #(3x1)\n",
    "bias_layer2 = np.array([[1],[2]]) #(2x1)\n",
    "bias_final = np.array([[7]])\n",
    "\n",
    "def hidden_layer1(inputs, weights, bias):\n",
    "   #(1x5)*(5x3) + (1x3), (inputs^T * weights) + bias^T\n",
    "   output = np.dot(inputs.T, weights) + bias.T #(1x3)\n",
    "   return np.maximum(0,output)\n",
    "\n",
    "def hidden_layer2(inputs, weights, bias):\n",
    "   #(1x3)*(3x2) + (1x3), (inputs^T * weights) + bias^T\n",
    "   output = np.dot(inputs, weights) + bias.T #(1x3)\n",
    "   return np.maximum(0,output)\n",
    "\n",
    "def final_layer(inputs, weights, bias):\n",
    "   #(2x1)*(1x1) + (1x1)\n",
    "   output = np.dot(inputs, weights) + bias\n",
    "   return np.maximum(0,output)\n",
    "print(\"Inputs:\\n\",input_original)\n",
    "print(\"Inputs for 1st layer:\\n\",hidden_layer1(input_original, weights_layer1, bias_layer1))\n",
    "inputs_2 = np.array(hidden_layer1(input_original, weights_layer1, bias_layer1))\n",
    "print(\"Inputs for 2nd layer:\\n\",hidden_layer2(inputs_2, weights_layer2, bias_layer2))\n",
    "inputs_final = np.array(hidden_layer2(inputs_2, weights_layer2, bias_layer2)) \n",
    "output_final = final_layer(inputs_final, weight_final, bias_final)\n",
    "print(\"Final output:\\n\",output_final.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Layered Back-Propagation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Forward Pass ==================\n",
      "1st Layer output:\n",
      " [[3.99 3.29 2.59]] \n",
      "\n",
      "2st Layer output:\n",
      " [[5.4552 6.4972]] \n",
      "\n",
      "Final Layer output:\n",
      " 7.188 \n",
      "\n",
      "================== Backward Pass ==================\n",
      "Error(Actual-Predicted):  [[-6.188368]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#        ================== Forward Pass ================== \n",
    "''' \n",
    "Feed Forward Multi Layer Neural Network:\n",
    "1. y = mx + c | no = w x i + b | neuron_output = weights x inputs + bias\n",
    "2. activation_function(neuron_output)\n",
    "'''\n",
    "\n",
    "#(3x1)\n",
    "inputs_original = np.array([[1],[2],[3]]) \n",
    "\n",
    "#(3x3)\n",
    "weights_layer1 = np.hstack([\n",
    "   np.array([[.3],[.34],[.67]]), # Weights for Neuron 1, (3x1)\n",
    "   np.array([[.2],[.23],[.21]]), # Weights for Neuron 2, (3x1)\n",
    "   np.array([[.4],[.43],[.11]]) # Weights for Neuron 3, (3x1)\n",
    "])\n",
    "\n",
    "#(3x2)\n",
    "weights_layer2 = np.hstack([\n",
    "   np.array([[.3],[.34],[.44]]), # Weights for Neuron 1, (3x1)\n",
    "   np.array([[.2],[.23],[.75]]), # Weights for Neuron 2, (3x1)\n",
    "])\n",
    "\n",
    "#(2x1)\n",
    "weight_final = np.array([[.33],[.66]]) # Weight for final Neuron\n",
    "\n",
    "bias_layer1 = np.array([[1],[2],[1]]) # Biases for Hidden 1st Layer, (3x1)\n",
    "bias_layer2 = np.array([[2],[3]]) # Biases for Hidden 2nd Layer, (2x1)\n",
    "bias_final = np.array([1.1]) # Bias for Final Layer, (1x1)\n",
    "\n",
    "def hidden_layer1(inputs, weights, bias):\n",
    "   # (3x1)*(3x3) + (1x3)\n",
    "   output = np.dot(inputs.T, weights) + bias.T\n",
    "   return np.maximum(0, output)\n",
    "\n",
    "def hidden_layer2(inputs, weights, bias):\n",
    "   # (1x3)*(3x2) + (1x2)\n",
    "   output = np.dot(inputs, weights) + bias.T\n",
    "   return np.maximum(0, output)\n",
    "\n",
    "def final_layer(inputs, weights, bias):\n",
    "   # (1x2)*(2x1) + (1x1)\n",
    "   output = np.dot(inputs, weights) + bias\n",
    "   return np.maximum(0, output)\n",
    "\n",
    "output_layer_1 = hidden_layer1(inputs_original, weights_layer1, bias_layer1)\n",
    "output_layer_2 = hidden_layer2(output_layer_1, weights_layer2, bias_layer2)\n",
    "final_layer = final_layer(output_layer_2, weight_final, bias_final)\n",
    "print(\"================== Forward Pass ==================\")\n",
    "print(\"1st Layer output:\\n\", output_layer_1,\"\\n\")\n",
    "print(\"2st Layer output:\\n\", output_layer_2,\"\\n\")\n",
    "print(\"Final Layer output:\\n\", final_layer.squeeze().round(3),'\\n')\n",
    "\n",
    "\n",
    "print(\"================== Backward Pass ==================\")  \n",
    "'''\n",
    "\n",
    "==================================== Back Propagation =====================================\n",
    "1. Loss Function (Mean Squared Error):\n",
    "   l = 1/2(a - p)^2 | loss = 0.5(actual - predicted)^2\n",
    "\n",
    "2. Gradients:\n",
    "Note: Weights, Biases, Final Output are derivated/unpacked to update wrt to gradient/changes/difference between actual & predicted.\n",
    "\n",
    "   [Final/Output Layer]\n",
    "      Loss w.r.t Output: (Loss Function Derivative)\n",
    "         dL/dp = (p - a) | loss_output = predicted - actual\n",
    "      Predicted w.r.t Weight:\n",
    "         dp/dw = A | predicted_weight = neuron_output\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = A x (dL/dp) | loss_output_weight = neuron_output x loss_output\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dp) | loss_bias = sum(loss_output)\n",
    "\n",
    "   [Hidden Layer]\n",
    "      Loss w.r.t  Pre-Activation Neuron Output (Z):\n",
    "         dL/dZ = dL/dA x dA/dZ = (dL/dp x w).ReLu(Z) | loss_raw_neuron_output = (loss_output x weight).activation_function(raw_neuron_output)\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = A x (dL/dZ) | loss_weight = neuron_output x (loss_raw_neuron_output)\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dZ) | loss_bias = sum(loss_raw_neuron_output)\n",
    "   \n",
    "   [Input Layer]\n",
    "      Loss w.r.t Pre-Activation Neuron Output (Z):\n",
    "         dL/dZ = (dL/dZ x w).ReLu(Z) | loss_raw_neuron_output = (loss_raw_neuron_output x weight).activation_function(raw_neuron_output)\n",
    "      Loss w.r.t Weight:\n",
    "         dL/dw = i x dL/dZ | loss_weight = input x loss_raw_neuron_output\n",
    "      Loss w.r.t Bias:\n",
    "         dL/db = sum(dL/dZ) | loss_bias = sum(loss_raw_neuron_output)\n",
    "\n",
    "   [Update Weights]\n",
    "      w = w - (lr x dL/dw) | weight = weight - learning_rate x loss_weight\n",
    "'''\n",
    "\n",
    "error = 1 - final_layer\n",
    "print(\"Error(Actual-Predicted): \",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
